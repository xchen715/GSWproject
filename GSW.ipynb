{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Modual\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set credential \n",
    "credentials = service_account.Credentials.from_service_account_file('D:/Google Drive/Courtney RA NBA/Keys/nba-prediction-wr-9a502b7df147.json')\n",
    "# Set credential Path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"D:/Google Drive/Courtney RA NBA/Keys/nba-prediction-wr-9a502b7df147.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload file into the GCP browser storage bucket\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    #Uploads a file to the bucket.\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse final score \n",
    "def final_score(ID):\n",
    "    # keep requesting\n",
    "    while True:\n",
    "        pbp_link = 'http://www.espn.com/nba/playbyplay?gameId=' + ID\n",
    "        pbp_r = requests.get(pbp_link) \n",
    "        pbp_soup = BeautifulSoup(pbp_r.content, 'html.parser')\n",
    "        pbptext = [s.getText() for s in pbp_soup.find_all('td') if s.get('class') != None and ('game-details'in s.get('class'))]\n",
    "        #when game ends\n",
    "        if  'End of Game' in pbptext:\n",
    "            # wait 120 seconds for espn to updati its box score page \n",
    "            time.sleep(150)\n",
    "            # pull scores from box score tab\n",
    "            stats_link = 'http://www.espn.com/nba/boxscore?gameId=' + ID\n",
    "            teamorder = pd.read_html(stats_link)[0]\n",
    "            break\n",
    "            \n",
    "    if teamorder.iloc[:,0][0] == 'GS':\n",
    "        home_score = teamorder.iloc[:,-1].values[0]\n",
    "        opp_score = teamorder.iloc[:,-1].values[1]\n",
    "    else:\n",
    "        home_score = teamorder.iloc[:,-1].values[1]\n",
    "        opp_score = teamorder.iloc[:,-1].values[0]\n",
    "            \n",
    "    # return score in string format '119-120'    \n",
    "    return str(home_score)+ '-' + str(opp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull half time box score \n",
    "def new_data_poper(ID):\n",
    "    while True:\n",
    "        pbp_link = 'http://www.espn.com/nba/playbyplay?gameId=' + ID\n",
    "        pbp_r = requests.get(pbp_link) \n",
    "        pbp_soup = BeautifulSoup(pbp_r.content, 'html.parser')\n",
    "        # \n",
    "        pbptext = [s.getText() for s in pbp_soup.find_all('td') if s.get('class') != None and ('game-details'in s.get('class'))]\n",
    "        if 'End of the 2nd Quarter' in pbptext:\n",
    "            # scrap half stats from box scores\n",
    "            stats_link = 'http://www.espn.com/nba/boxscore?gameId=' + ID\n",
    "            teamorder = pd.read_html(stats_link)[0]\n",
    "            first_team = pd.read_html(stats_link)[1]\n",
    "            second_team = pd.read_html(stats_link)[2]\n",
    "            break\n",
    "        \n",
    "    if teamorder.iloc[:,0][0] == 'GS':\n",
    "        home_team = first_team[first_team['Starters'] == 'TEAM']\n",
    "        opp_team = second_team[second_team['Starters'] == 'TEAM']\n",
    "    else:\n",
    "        home_team = second_team[second_team['Starters'] == 'TEAM']\n",
    "        opp_team = first_team[first_team['Starters'] == 'TEAM']\n",
    "    \n",
    "    # transform stats from \\d-\\d\\d format\n",
    "    for d in [home_team, opp_team]:\n",
    "        stats_extract(d)\n",
    "    \n",
    "    a = home_team[['3PTM','3PTA','FTA','FTM','OREB','DREB','REB','AST','STL','BLK','TO','PF','FGM','FGA']]\n",
    "    b = opp_team[['3PTM','3PTA','FTA','FTM','OREB','DREB','REB','AST','STL','BLK','TO','PF','FGM','FGA']]\n",
    "    new_data = pd.concat([a.reset_index(drop=True),b.reset_index(drop=True)],axis=1)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived columns from original box score table\n",
    "def stats_extract(df):\n",
    "    df['3PTA'] = df['3PT'].str.extract('-(\\d+)').values\n",
    "    df['3PTM'] = df['3PT'].str.extract('(\\d+)-').values\n",
    "    df['FTA'] = df['FT'].str.extract('-(\\d+)').values\n",
    "    df['FTM'] = df['FT'].str.extract('(\\d+)-').values\n",
    "    df['FGA'] = df['FG'].str.extract('-(\\d+)').values\n",
    "    df['FGM'] = df['FG'].str.extract('(\\d+)-').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to parse stats for history data for a team\n",
    "def parse_stats(df,side,col_side,col_opp):\n",
    "    df['threePM'+'_'+side] = np.where(df[col_side].str.contains(\"makes 3\",na=False).values,1,0)\n",
    "    df['threePA'+'_'+side] = np.where(df[col_side].str.contains(\"3-pt\",na=False).values,1,0)\n",
    "    df['FTA'+'_'+side] = np.where(df[col_side].str.contains(\"free\",na=False).values,1,0)\n",
    "    df['FTM'+'_'+side] = np.where(df[col_side].str.contains(\"(makes technical free)|(makes flagrant free)|(makes free) \",na=False,).values,1,0)\n",
    "    df['ORB'+'_'+side] = np.where(df[col_side].str.contains(\"[Oo]ffensive rebound\",na=False).values,1,0) - np.where(df[col_side].str.contains(\"[Oo]ffensive rebound by [Tt]eam\",na=False).values,1,0)\n",
    "    df['DRB'+'_'+side] = np.where(df[col_side].str.contains(\"[Dd]efensive rebound\",na=False).values,1,0) - np.where(df[col_side].str.contains(\"[Dd]efensive rebound by [Tt]eam\",na=False).values,1,0)\n",
    "    df['TRB'+'_'+side] = df['ORB'+'_'+side].values + df['DRB'+'_'+side].values\n",
    "    df['AST'+'_'+side] = np.where(df[col_side].str.contains(\"[Aa]ssist\",na=False).values,1,0)\n",
    "    df['STL'+'_'+side] = np.where(df[col_opp].str.contains(\"[Ss]teal\",na=False).values,1,0)\n",
    "    df['BLK'+'_'+side] = np.where(df[col_opp].str.contains(\"[Bb]lock\",na=False).values,1,0)\n",
    "    df['TOV'+'_'+side] = np.where(df[col_side].str.contains(\"[Tt]urnover\",na=False).values,1,0)\n",
    "    df['PF'+'_'+side] = np.where(df[col_opp].str.contains(\"([Ss]hooting foul)|([Tt]echnical foul)\",na=False).values,1,0) + np.where(df[col_side].str.contains(\"([Pp]ersonal foul)|([Cc]harge foul)\",na=False).values,1,0)\n",
    "    df['FG'+'_'+side] = np.where(df[col_side].str.contains(\"makes 2\",na=False).values,1,0)  + df['threePM'+'_'+side].values\n",
    "    df['FGA'+'_'+side] = np.where(df[col_side].str.contains(\"(misses 2)|(misses 3)\",na=False).values,1,0) + df['FG'+'_'+side].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to parse history data \n",
    "def team_parser(team_abb,start_years,end_years):\n",
    "    \n",
    "    domain = 'https://www.basketball-reference.com'\n",
    "    team = '/teams/' + team_abb +'/'\n",
    "    tail = '_games.html'\n",
    "    \n",
    "    # links of box score for each history game in the whole history\n",
    "    root_links = []\n",
    "    for year in range(start_years,end_years+1):\n",
    "        year = year\n",
    "        root_link = domain + team + str(year) + tail\n",
    "        root_links.append(root_link)\n",
    "        \n",
    "    # schedules for each history game\n",
    "    request_games = []\n",
    "    for root_link in root_links: \n",
    "        raw_schedule = pd.read_html(root_link, index_col=0)\n",
    "        if len(raw_schedule) > 1:\n",
    "            schedule = pd.concat([raw_schedule[0],raw_schedule[1]],axis=0)\n",
    "        else:\n",
    "            schedule = raw_schedule[0]\n",
    "        \n",
    "        schedule = schedule[schedule.index != 'G']\n",
    "        schedule.set_index('Date',inplace = True)\n",
    "    \n",
    "        game_date = schedule.index\n",
    "        game_date = game_date.astype(datetime.date)\n",
    "        \n",
    "        home_array = np.where(schedule.iloc[:,4].values == '@',0,1)\n",
    "    \n",
    "        # request html\n",
    "        r = requests.get(root_link)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        a_tags = soup.find_all('a')\n",
    "        \n",
    "        # store box score links \n",
    "        box_links = []\n",
    "        for link in a_tags:\n",
    "            if '/boxscores/' in str(link.get('href')) and \".html\" in str(link.get('href')):\n",
    "                box_links.append(domain + str(link.get('href')).replace('/boxscores/','/boxscores/pbp/'))\n",
    "        \n",
    "        #skip \"Last game link\"\n",
    "        if box_links[-1] == box_links[0]:\n",
    "            box_links = box_links[1:]\n",
    "        \n",
    "        # for each game\n",
    "        games = []\n",
    "        l = len(box_links)\n",
    "        for i in range(l):\n",
    "            pbp = pd.read_html(box_links[i], index_col=0)[0]\n",
    "        \n",
    "            # index of each quarter\n",
    "            idx_Q1 = list(pbp.index).index('1st Q')\n",
    "            idx_Q2 = list(pbp.index).index('2nd Q')\n",
    "            idx_Q3 = list(pbp.index).index('3rd Q')\n",
    "            idx_Q4 = list(pbp.index).index('4th Q')\n",
    "            pbp.reset_index(drop=True,inplace=True)\n",
    "            if home_array[i] == 1:\n",
    "                parse_stats(pbp,'team',5,1) # team stat\n",
    "                parse_stats(pbp,'opp',1,5) #opp stat\n",
    "\n",
    "            else:\n",
    "                parse_stats(pbp,'team',1,5) # team stat\n",
    "                parse_stats(pbp,'opp',5,1) #opp stat\n",
    "\n",
    "                \n",
    "            Q1 = pd.DataFrame(pbp.iloc[idx_Q1:idx_Q2, 5:].apply(sum,axis=0)).T\n",
    "            Q1.columns = ['Q1_'+col for col in Q1.columns]\n",
    "            Q2 = pd.DataFrame(pbp.iloc[idx_Q2:idx_Q3, 5:].apply(sum,axis=0)).T\n",
    "            Q2.columns = ['Q2_'+col for col in Q2.columns]\n",
    "            half = pd.DataFrame(pbp.iloc[idx_Q1:idx_Q3, 5:].apply(sum,axis=0)).T\n",
    "            half.columns = ['half_'+col for col in half.columns]\n",
    "            Q3 = pd.DataFrame(pbp.iloc[idx_Q3:idx_Q4, 5:].apply(sum,axis=0)).T\n",
    "            Q3.columns = ['Q3_'+col for col in Q3.columns]\n",
    "            first_3Q = pd.DataFrame(pbp.iloc[idx_Q1:idx_Q4, 5:].apply(sum,axis=0)).T\n",
    "            first_3Q.columns = ['first_3Q_'+col for col in first_3Q.columns]\n",
    "            Q4 = pd.DataFrame(pbp.iloc[idx_Q4:, 5:].apply(sum,axis=0)).T\n",
    "            Q4.columns = ['Q4_'+col for col in Q4.columns]\n",
    "            total = pd.DataFrame(pbp.iloc[:, 5:].apply(sum,axis=0)).T\n",
    "            total.columns = ['Total_'+col for col in total.columns]\n",
    "            \n",
    "            game_stats = pd.concat([Q1,Q2,half,Q3,first_3Q,Q4,total],axis=1)\n",
    "            games.append(game_stats)\n",
    "            \n",
    "        all_games = pd.concat(games)\n",
    "        all_games['scores_team'] = schedule['Tm'].values\n",
    "        all_games['scores_opp'] = schedule['Opp'].values\n",
    "        all_games = all_games.set_index(game_date)\n",
    "        request_games.append(all_games)\n",
    "    \n",
    "    hist_games = pd.concat(request_games)\n",
    "    \n",
    "    \n",
    "    return hist_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# parse history GSW history data \n",
    "d = team_parser('GSW',2015,2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_half = d[['half_threePM_team','half_threePA_team','half_FTA_team','half_FTM_team','half_ORB_team','half_DRB_team','half_TRB_team','half_AST_team','half_STL_team','half_BLK_team','half_TOV_team','half_PF_team','half_FG_team','half_FGA_team','half_threePM_opp','half_threePA_opp','half_FTA_opp','half_FTM_opp','half_ORB_opp','half_DRB_opp','half_TRB_opp','half_AST_opp','half_STL_opp','half_BLK_opp','half_TOV_opp','half_PF_opp','half_FG_opp','half_FGA_opp']]\n",
    "history_half = history_half.reset_index(drop=True)\n",
    "target = list(d['scores_team'].astype(int) - d['scores_opp'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=10, eps=0.001, fit_intercept=True,\n",
       "    max_iter=1000, n_alphas=1000, n_jobs=None, normalize=False,\n",
       "    positive=False, precompute='auto', random_state=None,\n",
       "    selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = LassoCV(n_alphas=1000,cv=10)\n",
    "ls.fit(history_half,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 schedule\n",
    "domain = 'https://www.basketball-reference.com'\n",
    "team = '/teams/' + 'GSW' +'/'\n",
    "tail = '_games.html'\n",
    "year = 2019\n",
    "root_link = domain + team + str(year) + tail\n",
    "\n",
    "schedule_GSW_2019 = pd.read_html(root_link)[0] \n",
    "\n",
    "schedule_GSW_2019 = schedule_GSW_2019[schedule_GSW_2019['G'] != 'G'][['Date','Start (ET)','Unnamed: 5','Opponent']]\n",
    "\n",
    "schedule_GSW_2019 = schedule_GSW_2019.rename(columns={'Unnamed: 5': 'Home','Start (ET)':'Start Time'})\n",
    "\n",
    "\n",
    "schedule_GSW_2019['Home'] = np.where(schedule_GSW_2019['Home'] == '@',False,True)\n",
    "time_split = schedule_GSW_2019['Start Time'].str[:-1].str.split(':')\n",
    "revised_time = time_split.map(lambda x: str(int(x[0]) + 12)+':' + x[1])\n",
    "\n",
    "schedule_GSW_2019['Start Date and Time'] = schedule_GSW_2019['Date'].values + ' ' + revised_time\n",
    "\n",
    "schedule_GSW_2019['Start Date and Time'] = schedule_GSW_2019['Start Date and Time'].map(lambda x: datetime.datetime.strptime(x,'%a, %b %d, %Y %H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get game Id\n",
    "l = 'http://www.espn.com/nba/team/schedule/_/name/gs'\n",
    "sch_r = requests.get(l) \n",
    "sch_soup = BeautifulSoup(sch_r.content, 'html.parser')\n",
    "\n",
    "list_gameId = []\n",
    "for gid in sch_soup.find_all('a'):\n",
    "    if (gid.get('href') != None) & ('http://www.espn.com/nba/game?gameId=' in str(gid.get('href'))):\n",
    "        list_gameId.append(str(gid.get('href')))\n",
    "gameIDs = pd.Series(list_gameId).str.extract('=(\\d+)')\n",
    "gameIDs.columns = ['IDs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.DataFrame({'Game Time':schedule_GSW_2019['Start Date and Time'], \"Team\":'Golden State Warriors', \"Opponent\": schedule_GSW_2019['Opponent'],'Predicted Final Points Differential':\"\", 'Final Scores':\"\" })\n",
    "# 0\n",
    "original.iloc[0,-2] = 8\n",
    "original.iloc[0,-1] = '108-100'\n",
    "# 1\n",
    "original.iloc[1,-2] = 10\n",
    "original.iloc[1,-1] = '124-123'\n",
    "# 2\n",
    "original.iloc[2,-2] = 10\n",
    "original.iloc[2,-1] = '98-100'\n",
    "# 3\n",
    "original.iloc[3,-2] = 9\n",
    "original.iloc[3,-1] = '123-103'\n",
    "# 4\n",
    "original.iloc[4,-2] = 10\n",
    "original.iloc[4,-1] = '144-122'\n",
    "# 5\n",
    "original.iloc[5,-2] = 11\n",
    "original.iloc[5,-1] = '128-100'\n",
    "# 6\n",
    "original.iloc[6,-2] = 9\n",
    "original.iloc[6,-1] = '120-114'\n",
    "# 7\n",
    "original.iloc[7,-2] = 12\n",
    "original.iloc[7,-1] = '149-124'\n",
    "# 8\n",
    "original.iloc[8,-2] = 10\n",
    "original.iloc[8,-1] = '131-121'\n",
    "# 9\n",
    "original.iloc[9,-2] = 8\n",
    "original.iloc[9,-1] = '116-99'\n",
    "# 10\n",
    "original.iloc[10,-2] = 7\n",
    "original.iloc[10,-1] = '117-101'\n",
    "#11\n",
    "original.iloc[11,-2] = 10\n",
    "original.iloc[11,-1] = '111-134'\n",
    "original.to_html('result.html',index=True)\n",
    "#12\n",
    "original.iloc[12,-2] = 8\n",
    "original.iloc[12,-1] = '116-100'\n",
    "original.to_html('result.html',index=True)\n",
    "#13\n",
    "original.iloc[13,-2] = 11\n",
    "original.iloc[13,-1] = '116-121'\n",
    "original.to_html('result.html',index=True)\n",
    "#14\n",
    "original.iloc[14,-2] = 10\n",
    "original.iloc[14,-1] = '110-103'\n",
    "original.to_html('result.html',index=True)\n",
    "\n",
    "upload_blob('nbagsw','result.html','result.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match time 2018-11-15 20:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Time                                2018-11-15 20:00:00\n",
      "Team                                   Golden State Warriors\n",
      "Opponent                                     Houston Rockets\n",
      "Predicted Final Points Differential                        5\n",
      "Final Scores                                     now playing\n",
      "Name: 15, dtype: object\n",
      "15 Ends\n"
     ]
    }
   ],
   "source": [
    "# Main Execution!\n",
    "\n",
    "\n",
    "games = len(original)\n",
    "for idx in range(15,games):\n",
    "    while True:\n",
    "        now = datetime.datetime.now()\n",
    "        schedule_time = original['Game Time'][idx]\n",
    "        if (now.year == schedule_time.year) & (now.month == schedule_time.month)  & (now.day == schedule_time.day) & (now.hour == schedule_time.hour) & (now.minute == schedule_time.minute):\n",
    "            print('match time',schedule_time)\n",
    "            original.loc[idx,'Predicted Final Points Differential'] = 'Start Predicting!'\n",
    "            original.loc[idx,'Final Scores'] = 'now playing'\n",
    "            original.to_html('result.html',index=True)\n",
    "            upload_blob('nbagsw','result.html','result.html')\n",
    "\n",
    "            ## activate parser of play by play and aggregate boxscores when hitting half time! store it in new_data\n",
    "            new_data = new_data_poper(str(gameIDs['IDs'][idx]))\n",
    "            #predict\n",
    "            pred = ls.predict(new_data)\n",
    "            \n",
    "            ## ouput the prediction to interface\n",
    "            insert_data =int(pred[0])\n",
    "            \n",
    "            original.loc[idx,'Predicted Final Points Differential'] = insert_data\n",
    "\n",
    "            #upload to bucket\n",
    "            original.to_html('result.html',index=True)\n",
    "            upload_blob('nbagsw','result.html','result.html')\n",
    "            print(original.loc[idx])\n",
    "            \n",
    "            ## after game ends paste the game results and data to dataframe\n",
    "            final = final_score(str(gameIDs['IDs'][idx]))\n",
    "            original.loc[idx,'Final Scores'] = final\n",
    "            #upload to bucket\n",
    "            original.to_html('result.html',index=True)\n",
    "            upload_blob('nbagsw','result.html','result.html')\n",
    "            \n",
    "            #save this game data for future\n",
    "            append_data = new_data.loc[0]\n",
    "            append_data.index = history_half.columns\n",
    "            history_half = history_half.append(append_data,ignore_index=True)\n",
    "            target_scores = re.split('-',final_score(str(gameIDs['IDs'][idx])))\n",
    "            append_target = int(target_scores[0]) - int(target_scores[1])\n",
    "            target.append(append_target)\n",
    "            \n",
    "            #retrain model\n",
    "            ls.fit(history_half,target)\n",
    "            \n",
    "            print(idx,'Ends')\n",
    "            ## exit this while loop\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
